# GLM-4

<p align="center">
 ğŸ“„<a href="https://arxiv.org/pdf/2406.12793" target="_blank"> ãƒ¬ãƒãƒ¼ãƒˆ </a> â€¢ ğŸ¤— <a href="https://huggingface.co/collections/THUDM/glm-4-665fcf188c414b03c2f7e3b7" target="_blank">HF ãƒªãƒã‚¸ãƒˆãƒª</a> â€¢ ğŸ¤– <a href="https://modelscope.cn/models/ZhipuAI/glm-4-9b-chat" target="_blank">ModelScope</a>  â€¢ ğŸŸ£ <a href="https://wisemodel.cn/models/ZhipuAI/glm-4-9b-chat" target="_blank">WiseModel</a>  â€¢ ğŸ¦ <a href="https://twitter.com/thukeg" target="_blank">Twitter</a> â€¢ ğŸ‘‹ <a href="https://discord.gg/8cnQKdAprg" target="_blank">Discord</a> ã¨ <a href="resources/WECHAT.md" target="_blank">WeChat</a> ã«å‚åŠ 
</p>
<p align="center">
ğŸ“<a href="https://open.bigmodel.cn/?utm_campaign=open&_channel_track_key=OWTVNma9">Zhipu AI ã‚ªãƒ¼ãƒ—ãƒ³ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ </a> ã§ã‚ˆã‚Šå¤§è¦æ¨¡ãª GLM ãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½“é¨“ãŠã‚ˆã³ä½¿ç”¨
</p>

[Englidsh](README.md) | [ä¸­æ–‡](README_zh.md) ã§èª­ã‚€

## æ›´æ–°æƒ…å ±

- ğŸ”¥ğŸ”¥ **ãƒ‹ãƒ¥ãƒ¼ã‚¹**: ```2024/11/01```: æœ¬ãƒªãƒã‚¸ãƒˆãƒªã®ä¾å­˜é–¢ä¿‚ãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸã€‚ãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ãå‹•ä½œã™ã‚‹ã‚ˆã†ã«ã€`requirements.txt` ã®ä¾å­˜é–¢ä¿‚ã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚ [glm-4-9b-chat-hf](https://huggingface.co/THUDM/glm-4-9b-chat-hf) ã®ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚§ã‚¤ãƒˆã¯ `transformers>=4.46.2` ã¨äº’æ›æ€§ãŒã‚ã‚Šã€`transformers` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã® `GlmModel` ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ã¦å®Ÿè£…ã§ãã¾ã™ã€‚ã¾ãŸã€ [glm-4-9b-chat](https://huggingface.co/THUDM/glm-4-9b-chat) ãŠã‚ˆã³ [glm-4v-9b](https://huggingface.co/THUDM/glm-4v-9b) ã® `tokenizer_chatglm.py` ãŒæœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã® `transformers` ã«å¯¾å¿œã™ã‚‹ã‚ˆã†ã«æ›´æ–°ã•ã‚Œã¾ã—ãŸã€‚HuggingFace ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚
- ğŸ”¥ **ãƒ‹ãƒ¥ãƒ¼ã‚¹**: ```2024/10/27```: [LongReward](https://github.com/THUDM/LongReward) ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã—ã¾ã—ãŸã€‚ã“ã‚Œã¯ã€AI ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¦é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’å¼·åŒ–ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
- ğŸ”¥ **ãƒ‹ãƒ¥ãƒ¼ã‚¹**: ```2024/10/25```: ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ä¸­å›½èª-è‹±èªéŸ³å£°å¯¾è©±ãƒ¢ãƒ‡ãƒ« [GLM-4-Voice](https://github.com/THUDM/GLM-4-Voice) ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã—ã¾ã—ãŸã€‚
- ğŸ”¥ **ãƒ‹ãƒ¥ãƒ¼ã‚¹**: ```2024/09/05```: é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã® Q&A ã§ LLM ãŒç´°ã‹ã„å¼•ç”¨ã‚’ç”Ÿæˆã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ãƒ¢ãƒ‡ãƒ« [longcite-glm4-9b](https://huggingface.co/THUDM/LongCite-glm4-9b) ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ [LongCite-45k](https://huggingface.co/datasets/THUDM/LongCite-45k) ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã—ã¾ã—ãŸã€‚ [Huggingface Space](https://huggingface.co/spaces/THUDM/LongCite) ã§ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚
- ğŸ”¥ **ãƒ‹ãƒ¥ãƒ¼ã‚¹**: ```2024/08/15```: å˜ä¸€ã‚¿ãƒ¼ãƒ³ã®å¯¾è©±ã§ 10,000 ãƒˆãƒ¼ã‚¯ãƒ³ä»¥ä¸Šã‚’ç”Ÿæˆã§ãã‚‹ãƒ¢ãƒ‡ãƒ« [longwriter-glm4-9b](https://huggingface.co/THUDM/LongWriter-glm4-9b) ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ [LongWriter-6k](https://huggingface.co/datasets/THUDM/LongWriter-6k) ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã—ã¾ã—ãŸã€‚ [Huggingface Space](https://huggingface.co/spaces/THUDM/LongWriter) ã¾ãŸã¯ [ModelScope Community Space](https://modelscope.cn/studios/ZhipuAI/LongWriter-glm4-9b-demo) ã§ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§ä½“é¨“ã—ã¦ãã ã•ã„ã€‚
- ğŸ”¥ **ãƒ‹ãƒ¥ãƒ¼ã‚¹**: ```2024/07/24```: é•·æ–‡å‡¦ç†ã«é–¢ã™ã‚‹æœ€æ–°ã®æŠ€è¡“çš„æ´å¯Ÿã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ GLM-4-9B ãƒ¢ãƒ‡ãƒ«ã®é•·æ–‡ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«é–¢ã™ã‚‹æŠ€è¡“ãƒ¬ãƒãƒ¼ãƒˆã‚’ [ã“ã¡ã‚‰](https://medium.com/@ChatGLM/glm-long-scaling-pre-trained-model-contexts-to-millions-caa3c48dea85) ã§ã”è¦§ãã ã•ã„ã€‚
- ğŸ”¥ **ãƒ‹ãƒ¥ãƒ¼ã‚¹**: ```2024/07/09```: GLM-4-9B-Chat ãƒ¢ãƒ‡ãƒ«ãŒ [Ollama](https://github.com/ollama/ollama) ã¨ [Llama.cpp](https://github.com/ggerganov/llama.cpp) ã«å¯¾å¿œã—ã¾ã—ãŸã€‚è©³ç´°ã¯ [PR](https://github.com/ggerganov/llama.cpp/pull/8031) ã‚’ã”è¦§ãã ã•ã„ã€‚
- ğŸ”¥ **ãƒ‹ãƒ¥ãƒ¼ã‚¹**: ```2024/06/18```: [æŠ€è¡“ãƒ¬ãƒãƒ¼ãƒˆ](https://arxiv.org/pdf/2406.12793) ã‚’å…¬é–‹ã—ã¾ã—ãŸã€‚ã”è¦§ãã ã•ã„ã€‚
- ğŸ”¥ **ãƒ‹ãƒ¥ãƒ¼ã‚¹**: ```2024/06/05```: GLM-4-9B ã‚·ãƒªãƒ¼ã‚ºã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸã€‚

## ãƒ¢ãƒ‡ãƒ«ç´¹ä»‹

GLM-4-9B ã¯ã€Zhipu AI ãŒãƒªãƒªãƒ¼ã‚¹ã—ãŸæœ€æ–°ä¸–ä»£ã® GLM-4 ã‚·ãƒªãƒ¼ã‚ºã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ç‰ˆã§ã™ã€‚ã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹ã€æ•°å­¦ã€æ¨è«–ã€ã‚³ãƒ¼ãƒ‰ã€çŸ¥è­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè©•ä¾¡ã«ãŠã„ã¦ã€**GLM-4-9B** ã¨ãã®äººé–“ã®å¥½ã¿ã«åˆã‚ã›ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ **GLM-4-9B-Chat** ã¯ã€Llama-3-8B ã‚’è¶…ãˆã‚‹å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ã¾ã—ãŸã€‚ãƒãƒ«ãƒãƒ©ã‚¦ãƒ³ãƒ‰ã®ä¼šè©±ã«åŠ ãˆã¦ã€GLM-4-9B-Chat ã¯ã€ã‚¦ã‚§ãƒ–ãƒ–ãƒ©ã‚¦ã‚¸ãƒ³ã‚°ã€ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œã€ã‚«ã‚¹ã‚¿ãƒ ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ï¼ˆFunction Callï¼‰ã€é•·æ–‡æ¨è«–ï¼ˆæœ€å¤§ 128K ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚µãƒãƒ¼ãƒˆï¼‰ãªã©ã®é«˜åº¦ãªæ©Ÿèƒ½ã‚‚å‚™ãˆã¦ã„ã¾ã™ã€‚ã“ã®ä¸–ä»£ã®ãƒ¢ãƒ‡ãƒ«ã¯å¤šè¨€èªã‚µãƒãƒ¼ãƒˆã‚’è¿½åŠ ã—ã€æ—¥æœ¬èªã€éŸ“å›½èªã€ãƒ‰ã‚¤ãƒ„èªã‚’å«ã‚€ 26 è¨€èªã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚ã¾ãŸã€1M ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ï¼ˆç´„ 200 ä¸‡æ–‡å­—ï¼‰ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ **GLM-4-9B-Chat-1M** ãƒ¢ãƒ‡ãƒ«ã¨ã€GLM-4-9B ã«åŸºã¥ããƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ« GLM-4V-9B ã‚‚ãƒªãƒªãƒ¼ã‚¹ã—ã¾ã—ãŸã€‚**GLM-4V-9B** ã¯ã€1120*1120 ã®é«˜è§£åƒåº¦ã§ã®ä¸­è‹±äºŒè¨€èªã®å¯¾è©±èƒ½åŠ›ã‚’æŒã¡ã€ä¸­è‹±ç·åˆèƒ½åŠ›ã€çŸ¥è¦šï¼†æ¨è«–ã€æ–‡å­—èªè­˜ã€ãƒãƒ£ãƒ¼ãƒˆç†è§£ãªã©ã®å¤šæ–¹é¢ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è©•ä¾¡ã«ãŠã„ã¦ã€GPT-4-turbo-2024-04-09ã€Gemini 1.0 Proã€Qwen-VL-Maxã€Claude 3 Opus ã‚’è¶…ãˆã‚‹å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ã¾ã—ãŸã€‚

## ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆ

|        ãƒ¢ãƒ‡ãƒ«        | ã‚¿ã‚¤ãƒ— | ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•· | Transformers ãƒãƒ¼ã‚¸ãƒ§ãƒ³ |                                                                                                      ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰                                                                                                       |                                                                                        ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¢                                                                                         |
|:-------------------:|:----:|:----------:|:--------------------:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|
|      GLM-4-9B       | ãƒ™ãƒ¼ã‚¹ |     8K     |  `4.44.0 - 4.45.0`   |             [ğŸ¤— Huggingface](https://huggingface.co/THUDM/glm-4-9b)<br> [ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/glm-4-9b)<br> [ğŸŸ£ WiseModel](https://wisemodel.cn/models/ZhipuAI/glm-4-9b)             |                                                                                             /                                                                                              |
|    GLM-4-9B-Chat    | ãƒãƒ£ãƒƒãƒˆ |    128K    |     `>= 4.44.0`      |     [ğŸ¤— Huggingface](https://huggingface.co/THUDM/glm-4-9b-chat)<br> [ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/glm-4-9b-chat)<br> [ğŸŸ£ WiseModel](https://wisemodel.cn/models/ZhipuAI/GLM-4-9B-Chat)      | [ğŸ¤– ModelScope CPU](https://modelscope.cn/studios/dash-infer/GLM-4-Chat-DashInfer-Demo/summary)<br> [ğŸ¤– ModelScope vLLM](https://modelscope.cn/studios/ZhipuAI/glm-4-9b-chat-vllm/summary) |
|  GLM-4-9B-Chat-HF   | ãƒãƒ£ãƒƒãƒˆ |    128K    |     `>= 4.46.0`      |                                     [ğŸ¤— Huggingface](https://huggingface.co/THUDM/glm-4-9b-chat-hf)<br> [ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/glm-4-9b-chat-hf)                                      | [ğŸ¤– ModelScope CPU](https://modelscope.cn/studios/dash-infer/GLM-4-Chat-DashInfer-Demo/summary)<br> [ğŸ¤– ModelScope vLLM](https://modelscope.cn/studios/ZhipuAI/glm-4-9b-chat-vllm/summary) |
|  GLM-4-9B-Chat-1M   | ãƒãƒ£ãƒƒãƒˆ |     1M     |     `>= 4.44.0`      | [ğŸ¤— Huggingface](https://huggingface.co/THUDM/glm-4-9b-chat-1m)<br> [ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/glm-4-9b-chat-1m)<br> [ğŸŸ£ WiseModel](https://wisemodel.cn/models/ZhipuAI/GLM-4-9B-Chat-1M) |                                                                                             /                                                                                              |
| GLM-4-9B-Chat-1M-HF | ãƒãƒ£ãƒƒãƒˆ |     1M     |     `>= 4.46.0`      |                                  [ğŸ¤— Huggingface](https://huggingface.co/THUDM/glm-4-9b-chat-1m-hf)<br> [ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/glm-4-9b-chat-1m-hf)                                   |                                                                                             /                                                                                              |
|      GLM-4V-9B      | ãƒãƒ£ãƒƒãƒˆ |     8K     |     `>= 4.46.0`      |           [ğŸ¤— Huggingface](https://huggingface.co/THUDM/glm-4v-9b)<br> [ğŸ¤– ModelScope](https://modelscope.cn/models/ZhipuAI/glm-4v-9b)<br> [ğŸŸ£ WiseModel](https://wisemodel.cn/models/ZhipuAI/GLM-4V-9B)            |                                                       [ğŸ¤– ModelScope](https://modelscope.cn/studios/ZhipuAI/glm-4v-9b-Demo/summary)                                                        |

## ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

### å…¸å‹çš„ãªã‚¿ã‚¹ã‚¯

| ãƒ¢ãƒ‡ãƒ«               | AlignBench | MT-Bench | IFEval | MMLU | C-Eval | GSM8K | MATH | HumanEval | NaturalCodeBench |
|:--------------------|:----------:|:--------:|:------:|:----:|:------:|:-----:|:----:|:---------:|:----------------:|
| Llama-3-8B-Instruct |    6.40    |   8.00   | 68.58  | 68.4 |  51.3  | 79.6  | 30.0 |   62.2    |       24.7       |
| ChatGLM3-6B         |    5.18    |   5.50   |  28.1  | 66.4 |  69.0  | 72.3  | 25.7 |   58.5    |       11.3       |
| GLM-4-9B-Chat       |    7.01    |   8.35   |  69.0  | 72.4 |  75.6  | 79.6  | 50.6 |   71.8    |       32.2       |

### ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«

| ãƒ¢ãƒ‡ãƒ«               | MMLU | C-Eval | GPQA | GSM8K | MATH | HumanEval |
|:--------------------|:----:|:------:|:----:|:-----:|:----:|:---------:|
| Llama-3-8B          | 66.6 |  51.2  |  -   | 45.8  |  -   |   33.5    |
| Llama-3-8B-Instruct | 68.4 |  51.3  | 34.2 | 79.6  | 30.0 |   62.2    |
| ChatGLM3-6B-Base    | 61.4 |  69.0  | 26.8 | 72.3  | 25.7 |   58.5    |
| GLM-4-9B            | 74.7 |  77.1  | 34.3 | 84.0  | 30.4 |   70.1    |

> `GLM-4-9B` ã¯ã€äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«æ•°å­¦ã€æ¨è«–ã€ã‚³ãƒ¼ãƒ‰ã«é–¢é€£ã™ã‚‹ä¸€éƒ¨ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¦ã„ã‚‹ãŸã‚ã€Llama-3-8B-Instruct ã‚‚æ¯”è¼ƒç¯„å›²ã«å«ã‚ã¦ã„ã¾ã™ã€‚

### é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

1M ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã§ [needle-in-the-haystack experiment](https://github.com/LargeWorldModel/LWM/blob/main/scripts/eval_needle.py) ã‚’å®Ÿæ–½ã—ã€çµæœã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š

![needle](resources/eval_needle.jpeg)

LongBench-Chat ã§é•·æ–‡èƒ½åŠ›ã‚’ã•ã‚‰ã«è©•ä¾¡ã—ã€çµæœã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š

<p align="center">
<img src="resources/longbench.png" alt="èª¬æ˜æ–‡" style="display: block; margin: auto; width: 65%;">
</p>

### å¤šè¨€èª

GLM-4-9B-Chat ã¨ Llama-3-8B-Instruct ã‚’ 6 ã¤ã®å¤šè¨€èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ†ã‚¹ãƒˆã—ã¾ã—ãŸã€‚ãƒ†ã‚¹ãƒˆçµæœã¨å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾å¿œã™ã‚‹è¨€èªã¯ä»¥ä¸‹ã®è¡¨ã®é€šã‚Šã§ã™ï¼š

| ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ     | Llama-3-8B-Instruct | GLM-4-9B-Chat |                                           è¨€èª                                            |
|:------------|:-------------------:|:-------------:|:----------------------------------------------------------------------------------------------:|
| M-MMLU      |        49.6         |     56.6      |                                              all                                               |
| FLORES      |        25.0         |     28.8      | ru, es, de, fr, it, pt, pl, ja, nl, ar, tr, cs, vi, fa, hu, el, ro, sv, uk, fi, ko, da, bg, no |
| MGSM        |        54.0         |     65.3      |                           zh, en, bn, de, es, fr, ja, ru, sw, te, th                           |
| XWinograd   |        61.7         |     73.1      |                                     zh, en, fr, jp, ru, pt                                     |
| XStoryCloze |        84.7         |     90.7      |                           zh, en, ar, es, eu, hi, id, my, ru, sw, te                           |
| XCOPA       |        73.3         |     80.1      |                           zh, et, ht, id, it, qu, sw, ta, th, tr, vi                           |

### é–¢æ•°å‘¼ã³å‡ºã—

[Berkeley Function Calling Leaderboard](https://github.com/ShishirPatil/gorilla/tree/main/berkeley-function-call-leaderboard) ã§ãƒ†ã‚¹ãƒˆã—ã¾ã—ãŸã€‚

| ãƒ¢ãƒ‡ãƒ«                  | å…¨ä½“ã®ç²¾åº¦ | AST ã‚µãƒãƒªãƒ¼ | å®Ÿè¡Œã‚µãƒãƒªãƒ¼ | é–¢é€£æ€§ |
|:-----------------------|:------------:|:-----------:|:------------:|:---------:|
| Llama-3-8B-Instruct    |    58.88     |    59.25    |    70.01     |   45.83   |
| gpt-4-turbo-2024-04-09 |    81.24     |    82.14    |    78.61     |   88.75   |
| ChatGLM3-6B            |    57.88     |    62.18    |    69.78     |   5.42    |
| GLM-4-9B-Chat          |    81.00     |    80.26    |    84.40     |   87.92   |

### ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«

GLM-4V-9B ã¯è¦–è¦šç†è§£èƒ½åŠ›ã‚’æŒã¤ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚é–¢é€£ã™ã‚‹ã‚¯ãƒ©ã‚·ãƒƒã‚¯ã‚¿ã‚¹ã‚¯ã®è©•ä¾¡çµæœã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š

|                            | **MMBench-EN-Test** | **MMBench-CN-Test** | **SEEDBench_IMG** | **MMStar** | **MMMU** | **MME** | **HallusionBench** | **AI2D** | **OCRBench** |
|----------------------------|---------------------|---------------------|-------------------|------------|----------|---------|--------------------|----------|--------------|
| **gpt-4o-2024-05-13**      | 83.4                | 82.1                | 77.1              | 63.9       | 69.2     | 2310.3  | 55                 | 84.6     | 736          |
| **gpt-4-turbo-2024-04-09** | 81.0                | 80.2                | 73.0              | 56.0       | 61.7     | 2070.2  | 43.9               | 78.6     | 656          |
| **gpt-4-1106-preview**     | 77.0                | 74.4                | 72.3              | 49.7       | 53.8     | 1771.5  | 46.5               | 75.9     | 516          |
| **InternVL-Chat-V1.5**     | 82.3                | 80.7                | 75.2              | 57.1       | 46.8     | 2189.6  | 47.4               | 80.6     | 720          |
| **LLaVA-Next-Yi-34B**      | 81.1                | 79                  | 75.7              | 51.6       | 48.8     | 2050.2  | 34.8               | 78.9     | 574          |
| **Step-1V**                | 80.7                | 79.9                | 70.3              | 50.0       | 49.9     | 2206.4  | 48.4               | 79.2     | 625          |
| **MiniCPM-Llama3-V2.5**    | 77.6                | 73.8                | 72.3              | 51.8       | 45.8     | 2024.6  | 42.4               | 78.4     | 725          |
| **Qwen-VL-Max**            | 77.6                | 75.7                | 72.7              | 49.5       | 52       | 2281.7  | 41.2               | 75.7     | 684          |
| **Gemini 1.0 Pro**         | 73.6                | 74.3                | 70.7              | 38.6       | 49       | 2148.9  | 45.7               | 72.9     | 680          |
| **Claude 3 Opus**          | 63.3                | 59.2                | 64                | 45.7       | 54.9     | 1586.8  | 37.8               | 70.6     | 694          |
| **GLM-4V-9B**              | 81.1                | 79.4                | 76.8              | 58.7       | 47.2     | 2163.8  | 46.6               | 81.1     | 786          |

## ã‚¯ã‚¤ãƒƒã‚¯ã‚³ãƒ¼ãƒ«

**ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ§‹æˆã¨ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶ã«ã¤ã„ã¦ã¯ã€[ã“ã¡ã‚‰](basic_demo/README_en.md) ã‚’ã”è¦§ãã ã•ã„ã€‚**

### GLM-4-9B-Chat è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’è¿…é€Ÿã«å‘¼ã³å‡ºã™æ–¹æ³•

transformers ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦æ¨è«–ã‚’è¡Œã†ï¼š

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

os.environ[
    'CUDA_VISIBLE_DEVICES'] = '0'  # GPU ç•ªå·ã‚’è¨­å®šã—ã¾ã™ã€‚è¤‡æ•°ã® GPU ã§æ¨è«–ã™ã‚‹å ´åˆã¯ã€è¤‡æ•°ã® GPU ç•ªå·ã‚’è¨­å®šã—ã¾ã™
MODEL_PATH = "THUDM/glm-4-9b-chat-hf"

device = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)

query = "ã“ã‚“ã«ã¡ã¯"

inputs = tokenizer.apply_chat_template([{"role": "user", "content": query}],
                                       add_generation_prompt=True,
                                       tokenize=True,
                                       return_tensors="pt",
                                       return_dict=True
                                       )

inputs = inputs.to(device)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    trust_remote_code=True,
    device_map="auto"
).eval()

gen_kwargs = {"max_length": 2500, "do_sample": True, "top_k": 1}
with torch.no_grad():
    outputs = model.generate(**inputs, **gen_kwargs)
    outputs = outputs[:, inputs['input_ids'].shape[1]:]
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

vLLM ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦æ¨è«–ã‚’è¡Œã†ï¼š

```python
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

# GLM-4-9B-Chat
# OOM ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€max_model_len ã‚’æ¸›ã‚‰ã™ã‹ã€tp_size ã‚’å¢—ã‚„ã—ã¦ã¿ã¦ãã ã•ã„
max_model_len, tp_size = 131072, 1
model_name = "THUDM/glm-4-9b-chat-hf"
prompt = [{"role": "user", "content": "ã“ã‚“ã«ã¡ã¯"}]

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
llm = LLM(
    model=model_name,
    tensor_parallel_size=tp_size,
    max_model_len=max_model_len,
    trust_remote_code=True,
    enforce_eager=True,
    # GLM-4-9B-Chat-1M ã§ OOM ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€ä»¥ä¸‹ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ‰åŠ¹ã«ã—ã¦ã¿ã¦ãã ã•ã„
    # enable_chunked_prefill=True,
    # max_num_batched_tokens=8192
)
stop_token_ids = [151329, 151336, 151338]
sampling_params = SamplingParams(temperature=0.95, max_tokens=1024, stop_token_ids=stop_token_ids)

inputs = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)
outputs = llm.generate(prompts=inputs, sampling_params=sampling_params)

print(outputs[0].outputs[0].text)

```

### GLM-4V-9B ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’è¿…é€Ÿã«å‘¼ã³å‡ºã™æ–¹æ³•

transformers ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦æ¨è«–ã‚’è¡Œã†ï¼š

```python
import torch
from PIL import Image
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

os.environ[
    'CUDA_VISIBLE_DEVICES'] = '0'  # GPU ç•ªå·ã‚’è¨­å®šã—ã¾ã™ã€‚è¤‡æ•°ã® GPU ã§æ¨è«–ã™ã‚‹å ´åˆã¯ã€è¤‡æ•°ã® GPU ç•ªå·ã‚’è¨­å®šã—ã¾ã™
MODEL_PATH = "THUDM/glm-4v-9b"

device = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)

query = 'ã“ã®ç”»åƒã®å†…å®¹ã‚’èª¬æ˜ã—ã¦ãã ã•ã„'
image = Image.open("your image").convert('RGB')
inputs = tokenizer.apply_chat_template([{"role": "user", "image": image, "content": query}],
                                       add_generation_prompt=True, tokenize=True, return_tensors="pt",
                                       return_dict=True)  # ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰

inputsã‚’ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€
inputs = inputs.to(device)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    trust_remote_code=True,
    device_map="auto"
).eval()

gen_kwargs = {"max_length": 2500, "do_sample": True, "top_k": 1}
with torch.no_grad():
    outputs = model.generate(**inputs, **gen_kwargs)
    outputs = outputs[:, inputs['input_ids'].shape[1]:]
    print(tokenizer.decode(outputs[0]))
```

vLLM ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦æ¨è«–ã‚’è¡Œã†ï¼š

```python
from PIL import Image
from vllm import LLM, SamplingParams

model_name = "THUDM/glm-4v-9b"

llm = LLM(model=model_name,
          tensor_parallel_size=1,
          max_model_len=8192,
          trust_remote_code=True,
          enforce_eager=True)
stop_token_ids = [151329, 151336, 151338]
sampling_params = SamplingParams(temperature=0.2,
                                 max_tokens=1024,
                                 stop_token_ids=stop_token_ids)

prompt = "ç”»åƒã®å†…å®¹ã¯ä½•ã§ã™ã‹ï¼Ÿ"
image = Image.open("your image").convert('RGB')
inputs = {
    "prompt": prompt,
    "multi_modal_data": {
        "image": image
    },
}
outputs = llm.generate(inputs, sampling_params=sampling_params)

for o in outputs:
    generated_text = o.outputs[0].text
    print(generated_text)

```

## å®Œå…¨ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªã‚¹ãƒˆ

GLM-4-9B ã‚·ãƒªãƒ¼ã‚ºã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„å ´åˆã€ã“ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒªãƒã‚¸ãƒˆãƒªã¯ã€ä»¥ä¸‹ã®å†…å®¹ã‚’é€šã˜ã¦é–‹ç™ºè€…ã«åŸºæœ¬çš„ãª GLM-4-9B ã®ä½¿ç”¨ãŠã‚ˆã³é–‹ç™ºã‚³ãƒ¼ãƒ‰ã‚’æä¾›ã—ã¾ã™

+ [basic_demo](basic_demo/README.md): å«ã¾ã‚Œã‚‹å†…å®¹
  + transformers ãŠã‚ˆã³ vLLM ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ãŸã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ãƒ‰
  + OpenAI API ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ãƒ‰
  + ãƒãƒƒãƒæ¨è«–ã‚³ãƒ¼ãƒ‰

+ [composite_demo](composite_demo/README.md): å«ã¾ã‚Œã‚‹å†…å®¹
  + GLM-4-9B ãŠã‚ˆã³ GLM-4V-9B ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®å®Œå…¨ãªæ©Ÿèƒ½ãƒ‡ãƒ¢ã‚³ãƒ¼ãƒ‰ã€All Tools æ©Ÿèƒ½ã€é•·æ–‡è§£é‡ˆã€ãŠã‚ˆã³ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ©Ÿèƒ½ã‚’å«ã‚€ã€‚

+ [fintune_demo](finetune_demo/README.md): å«ã¾ã‚Œã‚‹å†…å®¹
  + PEFT (LORA, P-Tuning) å¾®èª¿æ•´ã‚³ãƒ¼ãƒ‰
  + SFT å¾®èª¿æ•´ã‚³ãƒ¼ãƒ‰

+ [intel_device_demo](intel_device_demo/): å«ã¾ã‚Œã‚‹å†…å®¹
  + OpenVINO å±•é–‹ã‚³ãƒ¼ãƒ‰
  + IntelÂ® Extension for Transformers å±•é–‹ã‚³ãƒ¼ãƒ‰

## å‹å¥½çš„ãªãƒªãƒ³ã‚¯

+ [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory): é«˜åŠ¹ç‡ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹å¾®èª¿æ•´ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€GLM-4-9B-Chat è¨€èªãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚
+ [SWIFT](https://github.com/modelscope/swift): ModelScope ã® LLM/VLM ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€GLM-4-9B-Chat / GLM-4V-9b ã®å¾®èª¿æ•´ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚
+ [Xorbits Inference](https://github.com/xorbitsai/inference): ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Šã—ã€åŒ…æ‹¬çš„ãªã‚°ãƒ­ãƒ¼ãƒãƒ«æ¨è«–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ç‹¬è‡ªã®ãƒ¢ãƒ‡ãƒ«ã‚’ç°¡å˜ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ã‹ã€æœ€å…ˆç«¯ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¾ã™ã€‚
+ [LangChain-ChatChat](https://github.com/chatchat-space/Langchain-Chatchat): Langchain ã‚„ ChatGLM ãªã©ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ã RAG ãŠã‚ˆã³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
+ [self-llm](https://github.com/datawhalechina/self-llm/tree/master/models/GLM-4): Datawhale ã® self-llm ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€GLM-4-9B ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒƒã‚¯ãƒ–ãƒƒã‚¯ã‚’å«ã‚€ã€‚
+ [chatglm.cpp](https://github.com/li-plus/chatglm.cpp): é‡å­åŒ–ã«ã‚ˆã£ã¦ãƒ©ãƒƒãƒ—ãƒˆãƒƒãƒ—ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ã‚’å®Ÿç¾ã€llama.cpp ã«ä¼¼ã¦ã„ã¾ã™ã€‚
+ [OpenVINO](https://github.com/openvinotoolkit): glm-4-9b-chat ã¯ã™ã§ã« OpenVINO ã‚’ä½¿ç”¨ã—ã¦ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚ã“ã®ãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆã¯æ¨è«–ã‚’åŠ é€Ÿã—ã€Intel ã® GPUã€GPUã€ãŠã‚ˆã³ NPU ãƒ‡ãƒã‚¤ã‚¹ã§ã®æ¨è«–é€Ÿåº¦ã®å‘ä¸Šã‚’å®Ÿç¾ã—ã¾ã™ã€‚å…·ä½“çš„ãªä½¿ç”¨æ–¹æ³•ã«ã¤ã„ã¦ã¯ã€ [OpenVINO notebooks](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/notebooks/llm-chatbot/llm-chatbot-generate-api.ipynb) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

+ GLM-4 ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚§ã‚¤ãƒˆã®ä½¿ç”¨ã¯ã€ [ãƒ¢ãƒ‡ãƒ«ãƒ©ã‚¤ã‚»ãƒ³ã‚¹](https://huggingface.co/THUDM/glm-4-9b/blob/main/LICENSE) ã«å¾“ã†å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

+ ã“ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒªãƒã‚¸ãƒˆãƒªã®ã‚³ãƒ¼ãƒ‰ã¯ [Apache 2.0](LICENSE) ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«å¾“ã„ã¾ã™ã€‚

ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’å³å®ˆã—ã¦ãã ã•ã„ã€‚

## å¼•ç”¨

ç§ãŸã¡ã®ä»•äº‹ãŒå½¹ç«‹ã¤ã¨æ€ã‚ã‚Œã‚‹å ´åˆã¯ã€ä»¥ä¸‹ã®è«–æ–‡ã‚’å¼•ç”¨ã—ã¦ãã ã•ã„ã€‚

```
@misc{glm2024chatglm,
      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools},
      author={Team GLM  and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},
      year={2024},
      eprint={2406.12793},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}
```

```
@misc{wang2023cogvlm,
      title={CogVLM: Visual Expert for Pretrained Language Models},
      author={Weihan Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang},
      year={2023},
      eprint={2311.03079},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```
